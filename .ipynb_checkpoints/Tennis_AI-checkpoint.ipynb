{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#コメント\" data-toc-modified-id=\"コメント-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>コメント</a></span></li><li><span><a href=\"#import\" data-toc-modified-id=\"import-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>import</a></span></li><li><span><a href=\"#ゲームの位置とサイズ調整\" data-toc-modified-id=\"ゲームの位置とサイズ調整-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>ゲームの位置とサイズ調整</a></span></li><li><span><a href=\"#DQN\" data-toc-modified-id=\"DQN-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>DQN</a></span><ul class=\"toc-item\"><li><span><a href=\"#import\" data-toc-modified-id=\"import-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>import</a></span></li><li><span><a href=\"#GPU判定\" data-toc-modified-id=\"GPU判定-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>GPU判定</a></span></li><li><span><a href=\"#パラメータ\" data-toc-modified-id=\"パラメータ-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>パラメータ</a></span></li><li><span><a href=\"#関数\" data-toc-modified-id=\"関数-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>関数</a></span></li></ul></li><li><span><a href=\"#スレッド\" data-toc-modified-id=\"スレッド-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>スレッド</a></span><ul class=\"toc-item\"><li><span><a href=\"#画像取得\" data-toc-modified-id=\"画像取得-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>画像取得</a></span></li><li><span><a href=\"#スコア\" data-toc-modified-id=\"スコア-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>スコア</a></span></li><li><span><a href=\"#報酬\" data-toc-modified-id=\"報酬-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>報酬</a></span></li><li><span><a href=\"#録画\" data-toc-modified-id=\"録画-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>録画</a></span></li></ul></li><li><span><a href=\"#学習\" data-toc-modified-id=\"学習-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>学習</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# コメント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 報酬を-1, 0, 1に変更せず、やはりスコアで傾斜つけた方がいい。理由はは-1,0,1だと点数取っただけだと報酬に反映されず、学習が遅くなる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from Tennis_observation import *\n",
    "from Tennis_action import *\n",
    "from window_controlle import *\n",
    "import re\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "import time\n",
    "from win32 import win32gui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ゲームの位置とサイズ調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ゲーム画面の位置調整\n",
    "#1920×1200の画面において右上四分の一に配置\n",
    "win_left = 953\n",
    "win_top = 0\n",
    "win_width = 974\n",
    "win_height = 587"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab_screen\n",
    "#1920×1200の画面において右上四分の一に配置した画面のゲーム部分だけを取得\n",
    "left = 1087\n",
    "top = 30\n",
    "width = 705\n",
    "height = 545\n",
    "\n",
    "#少し大きめに画像をとってくる\n",
    "size_delta = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnnに渡すときの画像サイズ\n",
    "width_cnn = 84\n",
    "height_cnn = 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_frag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjust_window_pos_size(win_left, win_top, win_width, win_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = grab_screen(left, top, width, height, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU判定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "\n",
    "seed_value = 23\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習率\n",
    "learning_rate = 0.0001\n",
    "#ゲーム数\n",
    "num_episodes = 500\n",
    "gamma = 0.99\n",
    "\n",
    "hidden_layer = 512\n",
    "\n",
    "replay_mem_size = 100000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 5000\n",
    "\n",
    "double_dqn = True\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 10000\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 18\n",
    "\n",
    "clip_error = True\n",
    "normalize_image = True\n",
    "\n",
    "file2save = './model/tennis_save.pth'\n",
    "save_model_frequency = 10000\n",
    "resume_previous_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 0\n",
    "capacity = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = ['state', 'action', 'new_state', 'reward', 'done', 'info']\n",
    "\n",
    "number_of_inputs = frame.shape[0]\n",
    "number_of_outputs = len(keys_to_press)\n",
    "number_of_skips = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward\n",
    "init_flag = ('0.0','0.0')\n",
    "score_reward_dict = {'0.0':0, '15.0':1, '30.0':2, '40.0':3, '60.0':4, '99':99, 'nan':np.nan}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * math.exp(-1. * steps_done / egreedy_decay)\n",
    "    \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    return torch.load(file2save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    torch.save(model.state_dict(), file2save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame = frame.transpose((2,0,1))\n",
    "    frame = torch.from_numpy(frame)\n",
    "    frame = frame.to(device, dtype=torch.float32)\n",
    "    frame = frame.unsqueeze(1)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results():\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.title('Rewards')\n",
    "    plt.plot(reards_total, alpha=0.6, color='red')\n",
    "    plt.savefig('Tennis_result.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    " \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "        \n",
    "        self.position = ( self.position + 1 ) % self.capacity\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        \n",
    "        self.advantage1 = nn.Linear(7*7*64,hidden_layer)\n",
    "        self.advantage2 = nn.Linear(hidden_layer, number_of_outputs)\n",
    "        \n",
    "        self.value1 = nn.Linear(7*7*64,hidden_layer)\n",
    "        self.value2 = nn.Linear(hidden_layer,1)\n",
    "\n",
    "        #self.activation = nn.Tanh()\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if normalize_image:\n",
    "            x = x / 255\n",
    "        \n",
    "        output_conv = self.conv1(x)\n",
    "        output_conv = self.activation(output_conv)        \n",
    "        output_conv = self.conv2(output_conv)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        output_conv = self.conv3(output_conv)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        \n",
    "        output_conv = output_conv.view(output_conv.size(0), -1) # flatten\n",
    "        \n",
    "        output_advantage = self.advantage1(output_conv)\n",
    "        output_advantage = self.activation(output_advantage)\n",
    "        output_advantage = self.advantage2(output_advantage)\n",
    "        \n",
    "        output_value = self.value1(output_conv)\n",
    "        output_value = self.activation(output_value)\n",
    "        output_value = self.value2(output_value)\n",
    "        \n",
    "        output_final = output_value + output_advantage - output_advantage.mean()\n",
    "\n",
    "        return output_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.target_nn = NeuralNetwork().to(device)\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.number_of_frames = 0\n",
    "        \n",
    "        if resume_previous_training and os.path.exists(file2save):\n",
    "            print(\"Loading previously saved model ... \")\n",
    "            self.nn.load_state_dict(load_model())\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = preprocess_frame(state)\n",
    "                action_from_nn = self.nn(state)\n",
    "                \n",
    "                action = torch.max(action_from_nn,1)[1]\n",
    "                action = action.item()        \n",
    "        else: # random\n",
    "            action = action_random()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        \n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = [ preprocess_frame(frame) for frame in state ] \n",
    "        state = torch.cat(state)\n",
    "        \n",
    "        new_state = [ preprocess_frame(frame) for frame in new_state ] \n",
    "        new_state = torch.cat(new_state)\n",
    "\n",
    "        reward = Tensor(reward).to(device)\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "\n",
    "\n",
    "        if double_dqn:\n",
    "            new_state_indexes = self.nn(new_state).detach()\n",
    "            max_new_state_indexes = torch.max(new_state_indexes, 1)[1]  \n",
    "            \n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "        \n",
    "        \n",
    "        target_value = reward + ( 1 - done ) * gamma * max_new_state_values\n",
    "  \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1,1)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.number_of_frames % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        if self.number_of_frames % save_model_frequency == 0:\n",
    "            save_model(self.nn)\n",
    "        \n",
    "        self.number_of_frames += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スレッド"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画像取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_current = {'frame':grab_screen(left, top, width, height, False)}\n",
    "stop_flag = {'flag':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = threading.Thread(target=observation, args=(left, top, width, height, frame_current, stop_flag))\n",
    "t1.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スコア"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(my_score, enemy_score, stop_flag):\n",
    "    while True:\n",
    "        # score取得\n",
    "        score = score_check(frame_current['frame'])\n",
    "        my_score['my_score'] = float(score[0])\n",
    "        enemy_score['enemy_score'] = float(score[1])\n",
    "\n",
    "        if stop_flag['flag'] == True:\n",
    "            print('stop getting score')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_score = {'my_score':0}\n",
    "enemy_score = {'enemy_score':0}\n",
    "\n",
    "t2 = threading.Thread(target=get_score, args=(my_score, enemy_score, stop_flag))\n",
    "t2.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 報酬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_calculation(my_last_score, enemy_last_score, reward, stop_flag):\n",
    "    #スコアと報酬初期化\n",
    "    score = ('99', '99')\n",
    "    last_score = ('99', '99')\n",
    "    reward = -3\n",
    "    count_game = 0\n",
    "    \n",
    "    while stop_flag['flag'] != True:\n",
    "        #スコア取得\n",
    "        score = (str(my_score['my_score']), str(enemy_score['enemy_score']))\n",
    "        \n",
    "        #ポイント終了\n",
    "        if (score != last_score) & ('nan' not in score):\n",
    "            point_fin_flag['point_fin_flag'] = True\n",
    "            #print('score:', str(score))\n",
    "            \n",
    "            #(0,0)で報酬清算\n",
    "            #1ゲーム目の0-0は報酬計算対象外\n",
    "            if (score == init_flag) & (count_game == 0):\n",
    "                count_game += 1\n",
    "                #print('game: ', count_game)\n",
    "            \n",
    "            #2ゲーム目以降\n",
    "            if (score == init_flag) & (count_game > 0):\n",
    "                reward = score_reward_dict[last_score[0]] - score_reward_dict[last_score[1]]\n",
    "                #print('reward:', str(reward))\n",
    "                \n",
    "                count_game += 1\n",
    "                #print('game: ', count_game)\n",
    "            \n",
    "            #0-0の認識できなかった時の報酬計算処理\n",
    "            #if\n",
    "            \n",
    "\n",
    "            last_score = copy.copy(score)\n",
    "            my_last_score['my_last_score'] = last_score[0]\n",
    "            enemy_last_score['enemy_last_score'] = last_score[1]\n",
    "            \n",
    "    \n",
    "    print('fin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reward = {'reward':-3}\n",
    "my_last_score = {'my_last_score': '99'}\n",
    "enemy_last_score = {'enemy_last_score': '99'}\n",
    "\n",
    "\n",
    "t3 = threading.Thread(target=reward_calculation, args=(my_last_score, enemy_last_score, reward, stop_flag))\n",
    "t3.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(my_last_score['my_last_score'], enemy_last_score['enemy_last_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 録画"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "video_stop_flag = {'flag':False}\n",
    "t4 = Process(target=video_record, args=(frame_current['frame'], stop_flag)) # argsで引数二つ以上じゃないとエラー出る\n",
    "t4.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imshow(frame_current['frame'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_previous_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously saved model ... \n",
      "score: ('0.0', '0.0')\n",
      "game:  1\n",
      "reward: 0\n",
      "game:  2\n",
      "score: ('0.0', '15.0')\n",
      "score: ('0.0', '40.0')\n",
      "score: ('0.0', '0.0')\n",
      "reward: -3\n",
      "game:  3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-fb4cf51f2200>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m#ゲームが終わったかの確認\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mqnet_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;31m#ポイント終了\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-3cba0e9242ac>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mpreprocess_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-3cba0e9242ac>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mpreprocess_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-4807f25b3352>\u001b[0m in \u001b[0;36mpreprocess_frame\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "memory = ExperienceReplay(replay_mem_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "\n",
    "reward_list = []\n",
    "score_list = []\n",
    "\n",
    "frames_total = 0\n",
    "count_game = 0\n",
    "\n",
    "active_window()\n",
    "reward = 0.0\n",
    "\n",
    "done = False\n",
    "\n",
    "score = ('99', '99')\n",
    "last_score = ('99', '99')\n",
    "\n",
    "# 0-0になる数(ゲーム数)\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "\n",
    "    score_list_buf = []\n",
    "    reward_list_buf = []\n",
    "    \n",
    "    new_state = image_gray_resize(frame_current['frame'], width_cnn, height_cnn)\n",
    "    state = new_state\n",
    "    \n",
    "    # 1ゲーム終わるまで継続\n",
    "    while True:\n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        \n",
    "        #行動決定\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        \n",
    "        #行動\n",
    "        take_action(action)\n",
    "        \n",
    "        #新しい環境\n",
    "        state = new_state\n",
    "        \n",
    "        new_state = image_gray_resize(frame_current['frame'], width_cnn, height_cnn)\n",
    "        \n",
    "        score = (str(my_score['my_score']), str(enemy_score['enemy_score']))\n",
    "        \n",
    "        #ゲームが終わったかの確認               \n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        #ポイント終了\n",
    "        if (score != last_score) & ('nan' not in score):\n",
    "            print('score:', str(score))\n",
    "            \n",
    "            score_list_buf.append(score)\n",
    "            \n",
    "            #(0,0)で報酬清算\n",
    "            #1ゲーム目の0-0は報酬計算対象外\n",
    "            if (score == init_flag) & (count_game == 0):\n",
    "                count_game += 1\n",
    "                print('game: ', count_game)\n",
    "            \n",
    "            #2ゲーム目以降\n",
    "            if (score == init_flag) & (count_game > 0):\n",
    "                reward = score_reward_dict[last_score[0]] - score_reward_dict[last_score[1]]\n",
    "                reward_list.append(reward)\n",
    "                print('reward:', str(reward))\n",
    "                \n",
    "                count_game += 1\n",
    "                print('game: ', count_game)\n",
    "            \n",
    "            #0-0の認識できなかった時の報酬計算処理\n",
    "            #if\n",
    "\n",
    "            last_score = copy.copy(score)\n",
    "            break\n",
    "        \n",
    "        score_list.append(score_list_buf)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
